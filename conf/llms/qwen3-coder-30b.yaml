# Qwen3-Coder 30B A3B (MoE; strong coding) via local Ollama
model_name: "ollama/qwen3-coder:30b"
temperature: 0.0
max_tokens: 16384  # increased from 4096 - reasoning model needs more room
api_base: ${oc.env:LITELLM_API_BASE,"http://localhost:11434"}
api_key: ${oc.env:LITELLM_API_KEY,"ollama"}
supports_system_message: true
supports_thinking: true
supports_streaming: true
supports_vision: false
context_window: 128000
reasoning_capable: true
